{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes of Using BatchGenerators Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some simple notes and examples about how to use the BatchGenerators Modules, which may be more precise than the official help notebooks. \n",
    "\n",
    "1. The first part is about how to build the MultiThread Dataloader from our own data, which includes two different realizations: \n",
    "\n",
    "    **~ `batchgenerators.dataloading.data_loader.DataLoaderFromDataset`**\n",
    "\n",
    "    **~ `batchgenerators.dataloading.data_loader.DataLoader`**\n",
    "\n",
    "2. The second part is about some transformation (for augmentation) (to be finished)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MultiThread DataLoader\n",
    "\n",
    "### Via `batchgenerators.dataloading.data_loader.DataLoaderFromDataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples, we take the MNIST dataset as example. Instead of directly use\n",
    "```python\n",
    "    torchvision.datasets.MNIST\n",
    "```\n",
    "Here we use the original **xxx.gz** files to load the MNIST to get the adrray-type MNIST data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First is the function of loading MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import gzip\n",
    "import os\n",
    "\n",
    "def load_data(data_folder):\n",
    "    files = ['train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz']\n",
    "\n",
    "    paths = []\n",
    "    for fname in files:\n",
    "        paths.append(os.path.join(data_folder, fname))\n",
    "    with gzip.open(paths[0], 'rb') as imgpath:\n",
    "        x_train = np.frombuffer(\n",
    "            imgpath.read(), np.uint8, offset=16\n",
    "        ).reshape(-1, 28, 28)\n",
    "    with gzip.open(paths[1], 'rb') as lbpath:\n",
    "        y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n",
    "    return x_train, y_train\n",
    "\n",
    "x_train, y_train = load_data('./MNIST/raw')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "data_dict = {'data': x_train, 'labels': y_train.astype(np.int64)} \n",
    "# here the int label must be np.int64 type, otherwise there will be some mistakes for buliding the dataloader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referring to the official file **batchgenerators/examples/cifar.py** and the realization of `CifarDataset` module in batchgenerator, then we can write our own Dataset firstly, which should inherit the `Dataset` module in batchgenerator. \n",
    "\n",
    "In fact, this is almost the same as the general dataset, but replace the \n",
    "```python\n",
    "    torch.utils.data.Dataset\n",
    "``` \n",
    "by the \n",
    "```python\n",
    "    batchgenerators.dataloading.dataset.Dataset\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "from batchgenerators.dataloading.dataset import Dataset\n",
    "class OurOwnDataset(Dataset):\n",
    "    def __init__(self, data, train=True, transform=None):\n",
    "        super(OurOwnDataset, self).__init__()\n",
    "\n",
    "        self.data = data['data']\n",
    "        self.labels = data['labels']\n",
    "        \n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "\n",
    "        # here we omit the judgement of if_train, since we only use the training dataset in mnist\n",
    "    def __getitem__(self, item):\n",
    "        data_dict = {'data': self.data[item:item+1], 'label': self.labels[item]}\n",
    "        return data_dict\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "ds_mnist = OurOwnDataset(data_dict)\n",
    "print(ds_mnist.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Notice}: $ In the data_dict, we use the slice `self.data[item:item+1]` but not directly `self.data[item]`. And the difference is that, for `self.data[item]` the batch data in `MultiThreadedAugmenter` will be in shape of **(batch_size * 28, 28)** while if we use `self.data[item:item+1]`, the size of the batch data is **(batch_size, 28, 28)** in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the command\n",
    "```python\n",
    "    batchgenerators.dataloading.data_loader.DataLoaderFromDataset\n",
    "```\n",
    "to build the DataLoader from the Dataset, then we can use\n",
    "```python\n",
    "    batchgenerators.dataloading.multi_threaded_augmenter.MultiThreadedAugmenter\n",
    "```\n",
    "\n",
    "$\\color{red}{\\textbf{ATTENTION}}$: \n",
    "\n",
    "1. The parameter `num_processes` in `MultiTreadedAugmenter` must be the same as the parameter `num_threads_in_multithreaded` in `DataLoaderFromDataset`.\n",
    "2. The `int` type data must be `np.int64`, otherwise in the `MultiThreadedAugmenter` there will be some mistakes from the `default_collate` function in **data_loader.py** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 28, 28)\n",
      "[5 0 4 1 9]\n"
     ]
    }
   ],
   "source": [
    "from batchgenerators.dataloading.data_loader import DataLoaderFromDataset\n",
    "from batchgenerators.dataloading.multi_threaded_augmenter import MultiThreadedAugmenter\n",
    "\n",
    "num_threads_in_mt = 10\n",
    "dl_train = DataLoaderFromDataset(ds_mnist, 5, num_threads_in_mt, shuffle=False)    \n",
    "mt_dl_train = MultiThreadedAugmenter(dl_train, None, num_threads_in_mt)\n",
    "\n",
    "batch_dict = next(mt_dl_train)\n",
    "print(batch_dict['data'].shape)\n",
    "print(batch_dict['label'])\n",
    "mt_dl_train._finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the normal iteration to get the batch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, batch shape: (5, 28, 28), batch label: [5 0 4 1 9]\n",
      "iter: 1, batch shape: (5, 28, 28), batch label: [2 1 3 1 4]\n",
      "iter: 2, batch shape: (5, 28, 28), batch label: [3 5 3 6 1]\n",
      "iter: 3, batch shape: (5, 28, 28), batch label: [7 2 8 6 9]\n",
      "iter: 4, batch shape: (5, 28, 28), batch label: [4 0 9 1 1]\n",
      "iter: 5, batch shape: (5, 28, 28), batch label: [2 4 3 2 7]\n",
      "iter: 6, batch shape: (5, 28, 28), batch label: [3 8 6 9 0]\n",
      "iter: 7, batch shape: (5, 28, 28), batch label: [5 6 0 7 6]\n",
      "iter: 8, batch shape: (5, 28, 28), batch label: [1 8 7 9 3]\n",
      "iter: 9, batch shape: (5, 28, 28), batch label: [9 8 5 9 3]\n",
      "iter: 10, batch shape: (5, 28, 28), batch label: [3 0 7 4 9]\n",
      "iter: 11, batch shape: (5, 28, 28), batch label: [8 0 9 4 1]\n"
     ]
    }
   ],
   "source": [
    "for i, batch_dict in enumerate(mt_dl_train):\n",
    "    print(f'iter: {i}, batch shape: {batch_dict[\"data\"].shape}, batch label: {batch_dict[\"label\"]}')\n",
    "    if i > 10:\n",
    "        break\n",
    "mt_dl_train._finish() # in this way, the _finish() is not necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Notice}: $ Each time we finish our iteration in the `mt_dl` by `next(mt_dl)`, we must close it by the command `mt_dl._finish()` or reload the `mt_dl`. Otherwise, the next time we use the `mt_dl`, it will continue to iterate the batch from the last time and after the iteration times is larger than the length of it, there will be mistakes. See the following simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(NumDataset).__init__()\n",
    "        self.data = data\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "data_num = np.arange(1,11)\n",
    "ds_num =NumDataset(data_num)\n",
    "dl_num = DataLoaderFromDataset(ds_num, 7, num_threads_in_mt, shuffle=False) # the batch size is 7\n",
    "mt_dl_num = MultiThreadedAugmenter(dl_num, None, num_threads_in_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, data: [1 2 3 4 5 6 7]\n",
      "epoch: 1, iter: 0, data: [1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    for i in range(ds_num.__len__() // 7):\n",
    "        data = next(mt_dl_num)\n",
    "        print(f'epoch: {epoch}, iter: {i}, data: {data}')\n",
    "    mt_dl_num._finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, data: [1 2 3 4 5 6 7]\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ds_num\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m() \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m7\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(mt_dl_num)\n\u001b[1;32m      4\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, iter: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m, data: \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[39m# mt_dl_num._finish()\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nnunet/lib/python3.10/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py:212\u001b[0m, in \u001b[0;36mMultiThreadedAugmenter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_queue_ctr \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    211\u001b[0m         logging\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mMultiThreadedGenerator: finished data generation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_next_item()\n\u001b[1;32m    216\u001b[0m \u001b[39mreturn\u001b[39;00m item\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    for i in range(ds_num.__len__() // 7):\n",
    "        data = next(mt_dl_num)\n",
    "        print(f'epoch: {epoch}, iter: {i}, data: {data}')\n",
    "    # mt_dl_num._finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via `batchgenerators.dataloading.data_loader.DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can also build our own dataloader and then use the `MultiThreadedAugmenter` to get the `mt_dl`, which can be simply realized by overwrite the method `generate_train_batch` in the class of `batchgenerators.dataloading.data_loader.DataLoader`. Here is `OurOwnMNISTDataloader` on the mnist dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchgenerators.dataloading.data_loader import DataLoader\n",
    "class OurOwnMNISTDataloader(DataLoader):\n",
    "    def __init__(self, data, batch_size, num_threads_in_multithreaded=1, seed_for_shuffle=None, return_incomplete=False, shuffle=True, infinite=False, sampling_probabilities=None):\n",
    "        super().__init__(data, batch_size, num_threads_in_multithreaded, seed_for_shuffle, return_incomplete, shuffle, infinite, sampling_probabilities)\n",
    "        self.indices = list(range(len(data['data']))) # necessary\n",
    "        \n",
    "    def generate_train_batch(self):\n",
    "        idx = self.get_indices()\n",
    "        img_for_batch = [self._data['data'][i] for i in idx]\n",
    "        label_for_batch = [self._data['labels'][i] for i in idx]\n",
    "        \n",
    "        data = np.zeros((self.batch_size, 1, 28, 28))\n",
    "        data_label = np.zeros(self.batch_size, )\n",
    "        for i, (img, label) in enumerate(zip(img_for_batch, label_for_batch)):\n",
    "            data[i] = img.reshape(-1, 28, 28)\n",
    "            data_label[i] = label\n",
    "        return {'data': data, 'label': data_label}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1, 28, 28)\n",
      "[4. 3. 2. 7. 1.]\n"
     ]
    }
   ],
   "source": [
    "dl_mnist = OurOwnMNISTDataloader(data_dict, 5, num_threads_in_mt)\n",
    "mt_dl_mnist = MultiThreadedAugmenter(dl_mnist, None, num_threads_in_mt)\n",
    "data = next(mt_dl_mnist)\n",
    "\n",
    "print(data['data'].shape)\n",
    "print(data['label'])\n",
    "mt_dl_mnist._finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Notice}: $\n",
    "\n",
    "1. Our own dataloader inherits the class `DataLoader`, the attribute `data` and `batch_size` **do not need to define**, since the class `DataLoader` has the attribute `self._data = data` and `self.batch_size = batch_size`;\n",
    "\n",
    "2. In our own dataloader, we need to define the attribute `self.indices = len(data)`, which defines the length of our our data. This attribute can be called in the method `self.get_indices()` to generate random batch data each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation for Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be finished.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnunet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
